{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code adapted from https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33#:~:text=The%20Keras%20ResNet%20got%20to,to%20do%20with%20weight%20initializations.\n",
    "# import plaidml\n",
    "# import plaidml.keras\n",
    "# plaidml.install_backend()\n",
    "# import os\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "    \n",
    "import random\n",
    "random.seed(10)\n",
    "\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "# Clean Script\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.utils import load_img\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D,GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential,Model,load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# path_to_directory = 'imagedata/'\n",
    "\n",
    "\n",
    "# Collect paths to images based on label\n",
    "# The csv file is used so that there is no data leak depending on the machine\n",
    "import pandas as pd\n",
    "patientPaths = pd.read_csv('patientpaths.csv')['0'].to_list()\n",
    "# patientPaths= glob(os.path.join(path_to_directory, \"*\", \"\"))\n",
    "\n",
    "# print(patientPaths)\n",
    "nonCancerTrPaths = []\n",
    "nonCancerTestPaths = []\n",
    "cancerousTrPaths = []\n",
    "cancerousTestPaths = [] \n",
    "nonCancerValPaths1 = []\n",
    "cancerousValPaths1 = []\n",
    "nonCancerValPaths2 = []\n",
    "cancerousValPaths2 = []\n",
    "\n",
    "for i in range(0,168):\n",
    "    nonCancerTrPaths.extend(glob(os.path.join(patientPaths[i], \"0/\", \"*\")))\n",
    "    cancerousTrPaths.extend(glob(os.path.join(patientPaths[i], \"1/\", \"*\")))\n",
    "\n",
    "for i in range(168, 196):\n",
    "    nonCancerValPaths1.extend(glob(os.path.join(patientPaths[i], \"0/\", \"*\")))\n",
    "    cancerousValPaths1.extend(glob(os.path.join(patientPaths[i], \"1/\", \"*\")))\n",
    "    \n",
    "for i in range(196, 224):\n",
    "    nonCancerValPaths2.extend(glob(os.path.join(patientPaths[i], \"0/\", \"*\")))\n",
    "    cancerousValPaths2.extend(glob(os.path.join(patientPaths[i], \"1/\", \"*\")))\n",
    "\n",
    "for i in range(224,280):\n",
    "    nonCancerTestPaths.extend(glob(os.path.join(patientPaths[i], \"0/\", \"*\")))\n",
    "    cancerousTestPaths.extend(glob(os.path.join(patientPaths[i], \"1/\", \"*\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn filepaths into image arrays to train a model\n",
    "def paths_to_image(paths, label, num_samples):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for path in paths[0:num_samples]:\n",
    "        image = load_img(path)\n",
    "        image = image.resize([50, 50])\n",
    "        imgArray = tf.keras.utils.img_to_array(image)\n",
    "        images.append(imgArray)\n",
    "        labels.append(label)\n",
    "    return [images[0: num_samples], labels[0: num_samples]]\n",
    "\n",
    "nonCancerTrImages = paths_to_image(nonCancerTrPaths, 0, len(nonCancerTrPaths))\n",
    "cancerTrImages = paths_to_image(cancerousTrPaths, 1, len(cancerousTrPaths))\n",
    "\n",
    "nonCancerValImages1 = paths_to_image(nonCancerValPaths1, 0, len(nonCancerValPaths1))\n",
    "cancerValImages1 = paths_to_image(cancerousValPaths1, 1, len(cancerousValPaths1))\n",
    "\n",
    "nonCancerValImages2 = paths_to_image(nonCancerValPaths2, 0, len(nonCancerValPaths2))\n",
    "cancerValImages2 = paths_to_image(cancerousValPaths2, 1, len(cancerousValPaths2))\n",
    "\n",
    "nonCancerTestImages = paths_to_image(nonCancerTestPaths, 0, len(nonCancerTestPaths))\n",
    "cancerTestImages = paths_to_image(cancerousTestPaths, 1, len(cancerousTestPaths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cancer images in training set 119604\n",
      "Number of benign images in training set 119604\n"
     ]
    }
   ],
   "source": [
    "x_train = nonCancerTrImages[0] + cancerTrImages[0]\n",
    "y_train = nonCancerTrImages[1] + cancerTrImages[1]\n",
    "\n",
    "x_val1 = nonCancerValImages1[0] + cancerValImages1[0]\n",
    "y_val1 = nonCancerValImages1[1] + cancerValImages1[1]\n",
    "\n",
    "x_val2 = nonCancerValImages2[0] + cancerValImages2[0]\n",
    "y_val2 = nonCancerValImages2[1] + cancerValImages2[1]\n",
    "\n",
    "x_test = nonCancerTestImages[0]+ cancerTestImages[0]\n",
    "y_test = nonCancerTestImages[1]+ cancerTestImages[1]\n",
    "\n",
    "#function which rotates image a certain number of degrees = to rotation\n",
    "def rotate_image(X, y, rotation, num_to_rotate = None):\n",
    "    rotatedImages = []\n",
    "    labels = []\n",
    "    cancerX = X[y==1]\n",
    "    if num_to_rotate is not None and num_to_rotate < len(cancerX):\n",
    "        cancerX = cancerX[0: num_to_rotate]\n",
    "    for img in cancerX:\n",
    "        image = tf.keras.utils.array_to_img(img)\n",
    "        image = image.resize([50, 50])\n",
    "        rotateimg = image.rotate(rotation)\n",
    "        imgArray = tf.keras.utils.img_to_array(rotateimg)\n",
    "        rotatedImages.append(imgArray)\n",
    "        labels.append(1)\n",
    "    return [rotatedImages, labels]\n",
    "\n",
    "train90 = rotate_image(np.array(cancerTrImages[0]), np.array(cancerTrImages[1]), 90, 25758)\n",
    "train180 = rotate_image(np.array(cancerTrImages[0]), np.array(cancerTrImages[1]), 180, 25758)\n",
    "train270 = rotate_image(np.array(cancerTrImages[0]), np.array(cancerTrImages[1]), 270, 25757)\n",
    "\n",
    "print(\"Number of cancer images in training set\", str((len(train180[0]) + len(train90[0])) + len(train270[0]) + len(cancerTrImages[0])))\n",
    "print(\"Number of benign images in training set\", str(len(nonCancerTrImages[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cancer images in training set 119604\n",
      "Number of benign images in training set 119604\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of cancer images in training set\", str((len(train180[0]) + len(train90[0])) + len(train270[0]) + sum(y_train)))\n",
    "print(\"Number of benign images in training set\", str(len(y_train) - sum(y_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combining training data with rotated data\n",
    "x_train = np.array(x_train)\n",
    "train90[0] = np.array(train90[0])\n",
    "train180[0] = np.array(train180[0])\n",
    "y_train = np.array(y_train)\n",
    "train90[1] = np.array(train90[1])\n",
    "train180[1] = np.array(train180[1])\n",
    "\n",
    "training_x = np.concatenate((x_train, train90[0], train180[0],train270[0]), axis=0)\n",
    "training_y = np.concatenate((y_train, train90[1], train180[1],train270[1]))\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "datagen.fit(training_x)\n",
    "\n",
    "# Because of computing power problems we subset to 20000 images \n",
    "# (10000 cancerous and 10000 noncancerous) in training\n",
    "# train_x_param = np.concatenate((training_x[0:5000] ,training_x[120000:125000]), axis=0)\n",
    "# train_y_param = np.concatenate((training_y[0:5000] ,training_y[120000:125000]))\n",
    "\n",
    "# train_param_iterator = datagen.flow(np.array(train_x_param), np.array(train_y_param))\n",
    "\n",
    "val1_iterator = datagen.flow(np.array(x_val1), np.array(y_val1))\n",
    "val1x_iterator = datagen.flow(np.array(x_val1),batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Learning rates above .01 are not useful\n",
    "import keras.backend as K\n",
    "import tensorflow_addons as tfa\n",
    "from keras.utils.np_utils import to_categorical  \n",
    "\n",
    "\n",
    "# learning_rates = [.0001, .001, .01]\n",
    "# batchsizes = [32, 256]\n",
    "# mccs = []\n",
    "\n",
    "# for lr in learning_rates:\n",
    "#     for bs in batchsizes: \n",
    "#         model = VGG16(weights=None, include_top=True, input_shape= (50, 50,3), classes=2, classifier_activation='softmax')\n",
    "#         adam = Adam(learning_rate=lr)\n",
    "#         model.compile(optimizer= adam, loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "#         print(\"FITTING\")\n",
    "#         history = model.fit(train_param_iterator, epochs=15, batch_size = bs, validation_data=val_iterator)\n",
    "    \n",
    "#         y_pred = model.predict(val1x_iterator)\n",
    "#         print(y_pred)\n",
    "#         y_true = to_categorical(np.array(y_val1), num_classes=2)\n",
    "#         print(y_true)\n",
    "#         metric = tfa.metrics.MatthewsCorrelationCoefficient(num_classes=2)\n",
    "#         metric.update_state(y_true, y_pred)\n",
    "#         result = metric.result()\n",
    "#         result.numpy()\n",
    "#         mccs.append((lr, bs, result.numpy()))\n",
    "\n",
    "# The model outputs a value between 0 and 1, which can be thought of as a probability of being a cancer or not\n",
    "# model.predict to find the pseudo probability which is a measure of confidence\n",
    "# we can output the score and the prediction\n",
    "# activation map to find the feature\n",
    "\n",
    "\n",
    "# LR = .0001, BS = 256 achieves the highest validation MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(269019, 50, 50, 3)\n",
      "(269019,)\n"
     ]
    }
   ],
   "source": [
    "# Include Val1 in our training set as we have validation 2 to tune our threshold\n",
    "training_x = np.concatenate((training_x, np.array(x_val1)), axis=0)\n",
    "training_y = np.concatenate((training_y, np.array(y_val1)))\n",
    "\n",
    "print(training_x.shape)\n",
    "print(training_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FITTING\n",
      "Epoch 1/15\n",
      "8407/8407 [==============================] - 5872s 698ms/step - loss: 0.3117 - accuracy: 0.8642 - val_loss: 0.3799 - val_accuracy: 0.8335\n",
      "Epoch 2/15\n",
      "8407/8407 [==============================] - 5492s 653ms/step - loss: 0.2329 - accuracy: 0.9026 - val_loss: 0.3903 - val_accuracy: 0.8189\n",
      "Epoch 3/15\n",
      "8407/8407 [==============================] - 5587s 665ms/step - loss: 0.2129 - accuracy: 0.9107 - val_loss: 0.3611 - val_accuracy: 0.8409\n",
      "Epoch 4/15\n",
      "8407/8407 [==============================] - 5350s 636ms/step - loss: 0.2010 - accuracy: 0.9164 - val_loss: 0.3616 - val_accuracy: 0.8344\n",
      "Epoch 5/15\n",
      "8407/8407 [==============================] - 6040s 718ms/step - loss: 0.1895 - accuracy: 0.9212 - val_loss: 0.3403 - val_accuracy: 0.8520\n",
      "Epoch 6/15\n",
      "8407/8407 [==============================] - 6003s 714ms/step - loss: 0.1802 - accuracy: 0.9256 - val_loss: 0.3712 - val_accuracy: 0.8485\n",
      "Epoch 7/15\n",
      "8407/8407 [==============================] - 6176s 735ms/step - loss: 0.1723 - accuracy: 0.9286 - val_loss: 0.3245 - val_accuracy: 0.8549\n",
      "Epoch 8/15\n",
      "8407/8407 [==============================] - 6338s 754ms/step - loss: 0.1642 - accuracy: 0.9325 - val_loss: 0.3226 - val_accuracy: 0.8602\n",
      "Epoch 9/15\n",
      "8407/8407 [==============================] - 6306s 750ms/step - loss: 0.1545 - accuracy: 0.9364 - val_loss: 0.3418 - val_accuracy: 0.8483\n",
      "Epoch 10/15\n",
      "8407/8407 [==============================] - 6171s 734ms/step - loss: 0.1437 - accuracy: 0.9411 - val_loss: 0.3584 - val_accuracy: 0.8534\n",
      "Epoch 11/15\n",
      "8407/8407 [==============================] - 6187s 736ms/step - loss: 0.1320 - accuracy: 0.9462 - val_loss: 0.3545 - val_accuracy: 0.8589\n",
      "Epoch 12/15\n",
      "8407/8407 [==============================] - 6199s 737ms/step - loss: 0.1190 - accuracy: 0.9521 - val_loss: 0.3965 - val_accuracy: 0.8528\n",
      "Epoch 13/15\n",
      "8407/8407 [==============================] - 6283s 747ms/step - loss: 0.1057 - accuracy: 0.9577 - val_loss: 0.4201 - val_accuracy: 0.8533\n",
      "Epoch 14/15\n",
      "8407/8407 [==============================] - 6321s 752ms/step - loss: 0.0921 - accuracy: 0.9636 - val_loss: 0.4601 - val_accuracy: 0.8500\n",
      "Epoch 15/15\n",
      "8407/8407 [==============================] - 6282s 747ms/step - loss: 0.0787 - accuracy: 0.9696 - val_loss: 0.5418 - val_accuracy: 0.8472\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nICIiMUoKIiIS8/8DELtIiew3AGAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe400533128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ThresholdTuning/assets\n"
     ]
    }
   ],
   "source": [
    "# Refit the normalization/standardization to the new training set\n",
    "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "datagen.fit(training_x)\n",
    "\n",
    "train_iterator = datagen.flow(np.array(training_x), np.array(training_y))\n",
    "val2_iterator = datagen.flow(np.array(x_val2), np.array(y_val2))\n",
    "\n",
    "LR = .0001\n",
    "BS = 256\n",
    "model = VGG16(weights=None, include_top=True, input_shape= (50, 50,3), classes=2, classifier_activation='softmax')\n",
    "adam = Adam(learning_rate=LR)\n",
    "model.compile(optimizer= adam, loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "print(\"FITTING\")\n",
    "history = model.fit(train_iterator, epochs=15, validation_data=val2_iterator, batch_size=BS)\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.title(\"Learning Rate: \" + str(LR) + \" Batch Size: \" + str(BS))\n",
    "plt.show()\n",
    "\n",
    "model.save(\"ThresholdTuning\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeConfusionMatrix(preds, truth, threshold):\n",
    "    TPs = np.sum(truth[preds> threshold])\n",
    "    FPs = len(truth[preds > threshold]) - np.count_nonzero(truth[preds > threshold])\n",
    "    TNs = len(truth[preds <= threshold]) - np.count_nonzero(y_val2[preds <= threshold])\n",
    "    FNs = np.sum(truth[preds <= threshold])\n",
    "    \n",
    "    print(\"TP: \" + str(TPs / np.sum(preds> threshold)))\n",
    "    print(\"FP: \" + str(FPs / np.sum(preds> threshold)))\n",
    "    print(\"TN \" + str(TNs / np.sum(preds<=threshold)))\n",
    "    print(\"FN \" + str(FNs / np.sum(preds<=threshold)))\n",
    "    \n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "val2_iterator = datagen.flow(np.array(x_val2), np.array(y_val2))\n",
    "val2x_iterator = datagen.flow(np.array(x_val2), shuffle = False)\n",
    "\n",
    "val2_preds = model.predict(val2x_iterator)\n",
    "\n",
    "\n",
    "y_pred1 = [val2_preds[i][1] for i in range(0, val2_preds.shape[0])]\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(np.array(y_val2), y_pred1)\n",
    "\n",
    "\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "plt.figure()\n",
    "\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(\"ROCThresholdTuner.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 0.623337308503807\n",
      "FP: 0.376662691496193\n",
      "TN 0.9427159462657478\n",
      "FN 0.057284053734252105\n"
     ]
    }
   ],
   "source": [
    "Ts = np.linspace(0, 1, num=1000)\n",
    "ratio = 5\n",
    "numBenignMisses = []\n",
    "numCancerMisses = []\n",
    "\n",
    "predictions = np.array(y_pred1)\n",
    "y_val2 = np.array(y_val2)\n",
    "for thresh in Ts:\n",
    "    numBenignMiss = 0\n",
    "    numCancerMiss = 0\n",
    "    for i in range(0, len(predictions)):\n",
    "        if (predictions[i] >= thresh) and (y_val2[i] == 0):\n",
    "            numBenignMiss += 1\n",
    "        if (predictions[i] < thresh) and (y_val2[i] == 1):\n",
    "            numCancerMiss += 1\n",
    "    numBenignMisses.append(numBenignMiss)\n",
    "    numCancerMisses.append(numCancerMiss)\n",
    "\n",
    "minDiff = 1000000000\n",
    "Tstar = 0\n",
    "for i in range(0, len(Ts)):\n",
    "    diff = abs(numBenignMisses[i] - ratio*numCancerMisses[i])\n",
    "    if diff < minDiff:\n",
    "        minDiff = abs(numBenignMisses[i] - ratio*numCancerMisses[i])\n",
    "        Tstar = Ts[i]\n",
    "\n",
    "Tstar\n",
    "computeConfusionMatrix(predictions, y_val2, Tstar)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 0.6772732061953429\n",
      "FP: 0.322726793804657\n",
      "TN 0.924573746593142\n",
      "FN 0.07542625340685809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6428, 3063, 1190, 14587)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also find the optimal T for the posterior odds ratio using Bayesian detection theory\n",
    "posteriorRatio = val2_preds[:, 1] / val2_preds[:, 0]\n",
    "\n",
    "# The threshold is the ratio of the losses which is .1\n",
    "\n",
    "np.sum(posteriorRatio > .1)\n",
    "computeConfusionMatrix(posteriorRatio, y_val2, .1)\n",
    "\n",
    "# Compute the confusion matrix \n",
    "TPs = np.sum(y_val2[posteriorRatio > .1])\n",
    "FPs = len(y_val2[posteriorRatio > .1]) - np.count_nonzero(y_val2[posteriorRatio > .1])\n",
    "TNs = len(y_val2[posteriorRatio <= .1]) - np.count_nonzero(y_val2[posteriorRatio <= .1])\n",
    "FNs = np.sum(y_val2[posteriorRatio <= .1])\n",
    "\n",
    "TPs, FPs, FNs, TNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun on full training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model on full dataset \n",
    "\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "# Clean Script\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.utils import load_img\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D,GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential,Model,load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# path_to_directory = 'imagedata/'\n",
    "\n",
    "\n",
    "# Collect paths to images based on label\n",
    "# The csv file is used so that there is no data leak depending on the machine\n",
    "import pandas as pd\n",
    "patientPaths = pd.read_csv('patientpaths.csv')['0'].to_list()\n",
    "# patientPaths= glob(os.path.join(path_to_directory, \"*\", \"\"))\n",
    "\n",
    "# print(patientPaths)\n",
    "nonCancerTrPaths = []\n",
    "nonCancerTestPaths = []\n",
    "cancerousTrPaths = []\n",
    "cancerousTestPaths = [] \n",
    "nonCancerValPaths1 = []\n",
    "cancerousValPaths1 = []\n",
    "nonCancerValPaths2 = []\n",
    "cancerousValPaths2 = []\n",
    "\n",
    "for i in range(0,224):\n",
    "    nonCancerTrPaths.extend(glob(os.path.join(patientPaths[i], \"0/\", \"*\")))\n",
    "    cancerousTrPaths.extend(glob(os.path.join(patientPaths[i], \"1/\", \"*\")))\n",
    "\n",
    "for i in range(168, 196):\n",
    "    nonCancerValPaths1.extend(glob(os.path.join(patientPaths[i], \"0/\", \"*\")))\n",
    "    cancerousValPaths1.extend(glob(os.path.join(patientPaths[i], \"1/\", \"*\")))\n",
    "    \n",
    "for i in range(196, 224):\n",
    "    nonCancerValPaths2.extend(glob(os.path.join(patientPaths[i], \"0/\", \"*\")))\n",
    "    cancerousValPaths2.extend(glob(os.path.join(patientPaths[i], \"1/\", \"*\")))\n",
    "\n",
    "for i in range(224,280):\n",
    "    nonCancerTestPaths.extend(glob(os.path.join(patientPaths[i], \"0/\", \"*\")))\n",
    "    cancerousTestPaths.extend(glob(os.path.join(patientPaths[i], \"1/\", \"*\")))\n",
    "\n",
    "# Turn filepaths into image arrays to train a model\n",
    "def paths_to_image(paths, label, num_samples):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for path in paths[0:num_samples]:\n",
    "        image = load_img(path)\n",
    "        image = image.resize([50, 50])\n",
    "        imgArray = tf.keras.utils.img_to_array(image)\n",
    "        images.append(imgArray)\n",
    "        labels.append(label)\n",
    "    return [images[0: num_samples], labels[0: num_samples]]\n",
    "\n",
    "nonCancerTrImages = paths_to_image(nonCancerTrPaths, 0, len(nonCancerTrPaths))\n",
    "cancerTrImages = paths_to_image(cancerousTrPaths, 1, len(cancerousTrPaths))\n",
    "\n",
    "nonCancerValImages1 = paths_to_image(nonCancerValPaths1, 0, len(nonCancerValPaths1))\n",
    "cancerValImages1 = paths_to_image(cancerousValPaths1, 1, len(cancerousValPaths1))\n",
    "\n",
    "nonCancerValImages2 = paths_to_image(nonCancerValPaths2, 0, len(nonCancerValPaths2))\n",
    "cancerValImages2 = paths_to_image(cancerousValPaths2, 1, len(cancerousValPaths2))\n",
    "\n",
    "nonCancerTestImages = paths_to_image(nonCancerTestPaths, 0, len(nonCancerTestPaths))\n",
    "cancerTestImages = paths_to_image(cancerousTestPaths, 1, len(cancerousTestPaths))\n",
    "\n",
    "x_train = nonCancerTrImages[0] + cancerTrImages[0]\n",
    "y_train = nonCancerTrImages[1] + cancerTrImages[1]\n",
    "\n",
    "x_val1 = nonCancerValImages1[0] + cancerValImages1[0]\n",
    "y_val1 = nonCancerValImages1[1] + cancerValImages1[1]\n",
    "\n",
    "x_val2 = nonCancerValImages2[0] + cancerValImages2[0]\n",
    "y_val2 = nonCancerValImages2[1] + cancerValImages2[1]\n",
    "\n",
    "#function which rotates image a certain number of degrees = to rotation\n",
    "def rotate_image(X, y, rotation, num_to_rotate = None):\n",
    "    rotatedImages = []\n",
    "    labels = []\n",
    "    cancerX = X[y==1]\n",
    "    if num_to_rotate is not None and num_to_rotate < len(cancerX):\n",
    "        cancerX = cancerX[0: num_to_rotate]\n",
    "    for img in cancerX:\n",
    "        image = tf.keras.utils.array_to_img(img)\n",
    "        image = image.resize([50, 50])\n",
    "        rotateimg = image.rotate(rotation)\n",
    "        imgArray = tf.keras.utils.img_to_array(rotateimg)\n",
    "        rotatedImages.append(imgArray)\n",
    "        labels.append(1)\n",
    "    return [rotatedImages, labels]\n",
    "\n",
    "train90 = rotate_image(np.array(cancerTrImages[0]), np.array(cancerTrImages[1]), 90, 25758)\n",
    "train180 = rotate_image(np.array(cancerTrImages[0]), np.array(cancerTrImages[1]), 180, 25758)\n",
    "train270 = rotate_image(np.array(cancerTrImages[0]), np.array(cancerTrImages[1]), 270, 25757)\n",
    "\n",
    "val1aug90 = rotate_image(np.array(cancerValImages1[0]), np.array(cancerValImages1[1]), 90)\n",
    "val1aug180 = rotate_image(np.array(cancerValImages1[0]), np.array(cancerValImages1[1]), 90, 4776)\n",
    "val2aug90 = rotate_image(np.array(cancerValImages2[0]), np.array(cancerValImages2[1]), 90)\n",
    "val2aug180 = rotate_image(np.array(cancerValImages2[0]), np.array(cancerValImages2[1]), 90, 2414)\n",
    "\n",
    "print(\"Number of cancer images in training set\", str((len(train180[0]) + len(train90[0])) + len(train270[0]) + len(cancerTrImages[0])))\n",
    "print(\"Number of benign images in training set\", str(len(nonCancerTrImages[0])))\n",
    "\n",
    "print('cancer val images',len(cancerValImages1[0]))\n",
    "print('noncancer val images',len(nonCancerValImages1[0]))\n",
    "print('cancer val images',len(cancerValImages2[0]))\n",
    "print('noncancer val images',len(nonCancerValImages2[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining training data with rotated data\n",
    "x_train = np.array(x_train)\n",
    "train90[0] = np.array(train90[0])\n",
    "train180[0] = np.array(train180[0])\n",
    "y_train = np.array(y_train)\n",
    "train90[1] = np.array(train90[1])\n",
    "train180[1] = np.array(train180[1])\n",
    "\n",
    "training_x = np.concatenate((x_train, train90[0], train180[0],train270[0],x_val1,val1aug90[0],val1aug180[0],val2aug90[0],val2aug180[0]), axis=0)\n",
    "training_y = np.concatenate((y_train, train90[1], train180[1],train270[1],y_val1,val1aug90[1],val1aug180[1],val2aug90[1],val2aug180[1]))\n",
    "\n",
    "print(training_x.shape)\n",
    "print(training_y.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "datagen.fit(training_x)\n",
    "\n",
    "# Because of computing power problems we subset to 20000 images \n",
    "# (10000 cancerous and 10000 noncancerous) in training\n",
    "# train_x_param = np.concatenate((training_x[0:5000] ,training_x[120000:125000]), axis=0)\n",
    "# train_y_param = np.concatenate((training_y[0:5000] ,training_y[120000:125000]))\n",
    "\n",
    "# train_param_iterator = datagen.flow(np.array(train_x_param), np.array(train_y_param))\n",
    "\n",
    "val1_iterator = datagen.flow(np.array(x_val1), np.array(y_val1))\n",
    "val1x_iterator = datagen.flow(np.array(x_val1),batch_size=32, shuffle=False)\n",
    "\n",
    "# Refit the normalization/standardization to the new training set\n",
    "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "datagen.fit(training_x)\n",
    "\n",
    "train_iterator = datagen.flow(np.array(training_x), np.array(training_y))\n",
    "val2_iterator = datagen.flow(np.array(x_val2), np.array(y_val2))\n",
    "\n",
    "LR = .0001\n",
    "BS = 256\n",
    "model = VGG16(weights=None, include_top=True, input_shape= (50, 50,3), classes=2, classifier_activation='softmax')\n",
    "adam = Adam(learning_rate=LR)\n",
    "model.compile(optimizer= adam, loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "print(\"FITTING\")\n",
    "history = model.fit(train_iterator, epochs=15, validation_data=val2_iterator, batch_size=BS)\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.title(\"Learning Rate: \" + str(LR) + \" Batch Size: \" + str(BS))\n",
    "plt.show()\n",
    "\n",
    "model.save(\"ThresholdTuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_iterator = datagen.flow(np.array(x_test), np.array(y_test))\n",
    "\n",
    "model.evaluate(test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array(y_test)\n",
    "a[a==1].shape\n",
    " \n",
    "\n",
    "acc = (49684*0.4283320903778076+0.8424640893936157*19697) /(49684+19697)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_prediction = model.predict(np.array(x_test))\n",
    "# ypred2 = model.predict_classes(np.array(x_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = np.zeros(y_prediction.shape[0])\n",
    "mask = (y_prediction[:,0]<0.5)\n",
    "prediction[mask] = 1\n",
    "print(prediction)\n",
    "\n",
    "equal = np.zeros(y_prediction.shape[0])\n",
    "equal[prediction == np.array(y_test)] = 1\n",
    "\n",
    "accuracy = np.sum(equal)/y_prediction.shape[0]\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test[np.array(y_test) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "result = confusion_matrix(y_test, prediction , normalize='pred')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=result )\n",
    "\n",
    "disp.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = np.zeros(y_prediction.shape[0])\n",
    "mask = (y_prediction[:,1]<0.8)\n",
    "prediction[mask] = 1\n",
    "print(prediction)\n",
    "\n",
    "equal = np.zeros(y_prediction.shape[0])\n",
    "equal[prediction == np.array(y_test)] = 1\n",
    "\n",
    "accuracy = np.sum(equal)/y_prediction.shape[0]\n",
    "accuracy\n",
    "\n",
    "result = confusion_matrix(y_test, prediction , normalize='pred')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=result )\n",
    "\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cancer evaluation\n",
    "cancer_iterator = datagen.flow(np.array(np.array(x_test)[np.array(y_test) == 1]), np.array(y_test)[np.array(y_test) == 1])\n",
    "\n",
    "model.evaluate(cancer_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# benign evaluation\n",
    "\n",
    "benign_iterator = datagen.flow(np.array(np.array(x_test)[np.array(y_test) == 0]), np.array(y_test)[np.array(y_test) == 0])\n",
    "\n",
    "model.evaluate(benign_iterator)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## the penalty of missing a cancer is 2 million dollars * .05 100000\n",
    "## the penalty of diagnosing a cancer when it isn't there is lets say 20000\n",
    "import keras\n",
    "def get_img_array(img_path, size):\n",
    "    # `img` is a PIL image of size 299x299\n",
    "    img = load_img(img_path, target_size=size)\n",
    "    # `array` is a float32 Numpy array of shape (299, 299, 3)\n",
    "    array = tf.keras.utils.img_to_array(img)\n",
    "    # We add a dimension to transform our array into a \"batch\"\n",
    "    # of size (1, 299, 299, 3)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n",
    "plt.figure()\n",
    "img_size = (50, 50)\n",
    "path = \"imagedata/10253/0/10253_idx5_x1001_y1001_class0.png\"\n",
    "heatmap = make_gradcam_heatmap(datagen.flow(get_img_array(path, img_size))[0], model, \"block3_conv3\")\n",
    "plt.matshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n",
    "    # Load the original image\n",
    "    img = load_img(img_path)\n",
    "    img = tf.keras.utils.img_to_array(img)\n",
    "\n",
    "    # Rescale heatmap to a range 0-255\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    # Use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    # Create an image with RGB colorized heatmap\n",
    "    jet_heatmap = tf.keras.utils.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "\n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = jet_heatmap * alpha + img\n",
    "    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "\n",
    "    # Save the superimposed image\n",
    "    superimposed_img.save(cam_path)\n",
    "\n",
    "    # Display Grad CAM\n",
    "    display(Image(cam_path))\n",
    "\n",
    "\n",
    "save_and_display_gradcam(path, heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred_keras = model.predict(np.array(x_test)).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "y_pred0 = [y_prediction[i][1] for i in range(0, y_prediction.shape[0])]\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(np.array(y_test), y_pred0)\n",
    "\n",
    "\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "plt.figure()\n",
    "\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(\"ROC.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
