{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-02 11:11:54.804007: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# code adapted from https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33#:~:text=The%20Keras%20ResNet%20got%20to,to%20do%20with%20weight%20initializations.\n",
    "# import plaidml\n",
    "# import plaidml.keras\n",
    "# plaidml.install_backend()\n",
    "# import os\n",
    "# os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "    \n",
    "import random\n",
    "random.seed(10)\n",
    "\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "# Clean Script\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.utils import load_img\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D,GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential,Model,load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "path_to_directory = 'imagedata/'\n",
    "\n",
    "\n",
    "# Collect paths to images based on label\n",
    "patientPaths= glob(os.path.join(path_to_directory, \"*\", \"\"))\n",
    "\n",
    "# print(patientPaths)\n",
    "nonCancerTrPaths = []\n",
    "nonCancerTestPaths = []\n",
    "cancerousTrPaths = []\n",
    "cancerousTestPaths = [] \n",
    "nonCancerValPaths1 = []\n",
    "cancerousValPaths1 = []\n",
    "nonCancerValPaths2 = []\n",
    "cancerousValPaths2 = []\n",
    "\n",
    "for i in range(0,168):\n",
    "    nonCancerTrPaths.extend(glob(os.path.join(patientPaths[i], \"0/\", \"*\")))\n",
    "    cancerousTrPaths.extend(glob(os.path.join(patientPaths[i], \"1/\", \"*\")))\n",
    "\n",
    "for i in range(168, 196):\n",
    "    nonCancerValPaths1.extend(glob(os.path.join(patientPaths[i], \"0/\", \"*\")))\n",
    "    cancerousValPaths1.extend(glob(os.path.join(patientPaths[i], \"1/\", \"*\")))\n",
    "    \n",
    "for i in range(196, 224):\n",
    "    nonCancerValPaths2.extend(glob(os.path.join(patientPaths[i], \"0/\", \"*\")))\n",
    "    cancerousValPaths2.extend(glob(os.path.join(patientPaths[i], \"1/\", \"*\")))\n",
    "\n",
    "for i in range(224,280):\n",
    "    nonCancerTestPaths.extend(glob(os.path.join(patientPaths[i], \"0/\", \"*\")))\n",
    "    cancerousTestPaths.extend(glob(os.path.join(patientPaths[i], \"1/\", \"*\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn filepaths into image arrays to train a model\n",
    "def paths_to_image(paths, label, num_samples):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for path in paths[0:num_samples]:\n",
    "        image = load_img(path)\n",
    "        image = image.resize([50, 50])\n",
    "        imgArray = tf.keras.utils.img_to_array(image)\n",
    "        images.append(imgArray)\n",
    "        labels.append(label)\n",
    "    return [images[0: num_samples], labels[0: num_samples]]\n",
    "\n",
    "nonCancerTrImages = paths_to_image(nonCancerTrPaths, 0, len(nonCancerTrPaths))\n",
    "cancerTrImages = paths_to_image(cancerousTrPaths, 1, len(cancerousTrPaths))\n",
    "\n",
    "nonCancerValImages1 = paths_to_image(nonCancerValPaths1, 0, len(nonCancerValPaths1))\n",
    "cancerValImages1 = paths_to_image(cancerousValPaths1, 1, len(cancerousValPaths1))\n",
    "\n",
    "nonCancerValImages2 = paths_to_image(nonCancerValPaths2, 0, len(nonCancerValPaths2))\n",
    "cancerValImages2 = paths_to_image(cancerousValPaths2, 1, len(cancerousValPaths2))\n",
    "\n",
    "nonCancerTestImages = paths_to_image(nonCancerTestPaths, 0, len(nonCancerTestPaths))\n",
    "cancerTestImages = paths_to_image(cancerousTestPaths, 1, len(cancerousTestPaths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cancer images in training set 119604\n",
      "Number of benign images in training set 119604\n"
     ]
    }
   ],
   "source": [
    "x_train = nonCancerTrImages[0] + cancerTrImages[0]\n",
    "y_train = nonCancerTrImages[1] + cancerTrImages[1]\n",
    "\n",
    "x_val1 = nonCancerValImages1[0] + cancerValImages1[0]\n",
    "y_val1 = nonCancerValImages1[1] + cancerValImages1[1]\n",
    "\n",
    "x_val2 = nonCancerValImages2[0] + cancerValImages2[0]\n",
    "y_val2 = nonCancerValImages2[1] + cancerValImages2[1]\n",
    "\n",
    "x_test = nonCancerTestImages[0]+ cancerTestImages[0]\n",
    "y_test = nonCancerTestImages[1]+ cancerTestImages[1]\n",
    "\n",
    "#function which rotates image a certain number of degrees = to rotation\n",
    "def rotate_image(X, y, rotation, num_to_rotate = None):\n",
    "    rotatedImages = []\n",
    "    labels = []\n",
    "    cancerX = X[y==1]\n",
    "    if num_to_rotate is not None and num_to_rotate < len(cancerX):\n",
    "        cancerX = cancerX[0: num_to_rotate]\n",
    "    for img in cancerX:\n",
    "        image = tf.keras.utils.array_to_img(img)\n",
    "        image = image.resize([50, 50])\n",
    "        rotateimg = image.rotate(rotation)\n",
    "        imgArray = tf.keras.utils.img_to_array(rotateimg)\n",
    "        rotatedImages.append(imgArray)\n",
    "        labels.append(1)\n",
    "    return [rotatedImages, labels]\n",
    "42331\n",
    "\n",
    "train90 = rotate_image(np.array(cancerTrImages[0]), np.array(cancerTrImages[1]), 90, 25758)\n",
    "train180 = rotate_image(np.array(cancerTrImages[0]), np.array(cancerTrImages[1]), 180, 25758)\n",
    "train270 = rotate_image(np.array(cancerTrImages[0]), np.array(cancerTrImages[1]), 270, 25757)\n",
    "\n",
    "print(\"Number of cancer images in training set\", str((len(train180[0]) + len(train90[0])) + len(train270[0]) + len(cancerTrImages[0])))\n",
    "print(\"Number of benign images in training set\", str(len(nonCancerTrImages[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining training data with rotated data\n",
    "x_train = np.array(x_train)\n",
    "train90[0] = np.array(train90[0])\n",
    "train180[0] = np.array(train180[0])\n",
    "y_train = np.array(y_train)\n",
    "train90[1] = np.array(train90[1])\n",
    "train180[1] = np.array(train180[1])\n",
    "\n",
    "training_x = np.concatenate((x_train, train90[0], train180[0],train270[0]), axis=0)\n",
    "training_y = np.concatenate((y_train, train90[1], train180[1],train270[1]))\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "datagen.fit(training_x)\n",
    "\n",
    "# Because of computing power problems we subset to 20000 images \n",
    "# (10000 cancerous and 10000 noncancerous) in training\n",
    "train_x_param = np.concatenate((training_x[0:5000] ,training_x[120000:125000]), axis=0)\n",
    "train_y_param = np.concatenate((training_y[0:5000] ,training_y[120000:125000]))\n",
    "\n",
    "train_param_iterator = datagen.flow(np.array(train_x_param), np.array(train_y_param))\n",
    "\n",
    "val_iterator = datagen.flow(np.array(x_val1), np.array(y_val1))\n",
    "testx_iterator = datagen.flow(np.array(x_val1),batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amykim/opt/anaconda3/envs/cancerImages/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "2023-05-02 11:14:59.652263: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FITTING\n",
      "Epoch 1/15\n",
      "313/313 [==============================] - 472s 2s/step - loss: 0.5872 - accuracy: 0.6649 - val_loss: 0.8018 - val_accuracy: 0.6570\n",
      "Epoch 2/15\n",
      "313/313 [==============================] - 465s 1s/step - loss: 0.4430 - accuracy: 0.7972 - val_loss: 0.4765 - val_accuracy: 0.8130\n",
      "Epoch 3/15\n",
      "313/313 [==============================] - 468s 1s/step - loss: 0.4204 - accuracy: 0.8139 - val_loss: 0.5863 - val_accuracy: 0.7778\n",
      "Epoch 4/15\n",
      "313/313 [==============================] - 466s 1s/step - loss: 0.3893 - accuracy: 0.8287 - val_loss: 0.4852 - val_accuracy: 0.8158\n",
      "Epoch 5/15\n",
      "313/313 [==============================] - 466s 1s/step - loss: 0.3865 - accuracy: 0.8318 - val_loss: 0.6675 - val_accuracy: 0.7370\n",
      "Epoch 6/15\n",
      "313/313 [==============================] - 461s 1s/step - loss: 0.3538 - accuracy: 0.8467 - val_loss: 0.4871 - val_accuracy: 0.8007\n",
      "Epoch 7/15\n",
      "313/313 [==============================] - 461s 1s/step - loss: 0.3117 - accuracy: 0.8687 - val_loss: 0.4943 - val_accuracy: 0.7893\n",
      "Epoch 8/15\n",
      "313/313 [==============================] - 462s 1s/step - loss: 0.2523 - accuracy: 0.8984 - val_loss: 0.8758 - val_accuracy: 0.7193\n",
      "Epoch 9/15\n",
      "313/313 [==============================] - 465s 1s/step - loss: 0.2218 - accuracy: 0.9128 - val_loss: 0.7264 - val_accuracy: 0.7118\n",
      "Epoch 10/15\n",
      "313/313 [==============================] - 465s 1s/step - loss: 0.1830 - accuracy: 0.9290 - val_loss: 0.8763 - val_accuracy: 0.7181\n",
      "Epoch 11/15\n",
      "313/313 [==============================] - 466s 1s/step - loss: 0.1871 - accuracy: 0.9297 - val_loss: 0.9993 - val_accuracy: 0.6673\n",
      "Epoch 12/15\n",
      "313/313 [==============================] - 466s 1s/step - loss: 0.1899 - accuracy: 0.9271 - val_loss: 0.9850 - val_accuracy: 0.7009\n",
      "Epoch 13/15\n",
      "313/313 [==============================] - 467s 1s/step - loss: 0.1450 - accuracy: 0.9452 - val_loss: 0.6338 - val_accuracy: 0.7830\n",
      "Epoch 14/15\n",
      "313/313 [==============================] - 466s 1s/step - loss: 0.1298 - accuracy: 0.9510 - val_loss: 0.8546 - val_accuracy: 0.6999\n",
      "Epoch 15/15\n",
      "313/313 [==============================] - 466s 1s/step - loss: 0.1183 - accuracy: 0.9575 - val_loss: 0.7833 - val_accuracy: 0.7398\n",
      "932/932 [==============================] - 203s 218ms/step\n",
      "[[0.96234804 0.03765191]\n",
      " [0.15123175 0.84876823]\n",
      " [0.97943753 0.02056245]\n",
      " ...\n",
      " [0.81688446 0.18311554]\n",
      " [0.9949162  0.00508386]\n",
      " [0.5547919  0.4452081 ]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "FITTING\n",
      "Epoch 1/15\n",
      "313/313 [==============================] - 467s 1s/step - loss: 0.6938 - accuracy: 0.4975 - val_loss: 0.6930 - val_accuracy: 0.7201\n",
      "Epoch 2/15\n",
      "313/313 [==============================] - 466s 1s/step - loss: 0.6932 - accuracy: 0.4962 - val_loss: 0.6936 - val_accuracy: 0.2799\n",
      "Epoch 3/15\n",
      "313/313 [==============================] - 467s 1s/step - loss: 0.6928 - accuracy: 0.5080 - val_loss: 0.6875 - val_accuracy: 0.7201\n",
      "Epoch 4/15\n",
      "313/313 [==============================] - 467s 1s/step - loss: 0.6738 - accuracy: 0.5820 - val_loss: 0.6726 - val_accuracy: 0.7347\n",
      "Epoch 5/15\n",
      "313/313 [==============================] - 468s 1s/step - loss: 0.4632 - accuracy: 0.7815 - val_loss: 0.5245 - val_accuracy: 0.7782\n",
      "Epoch 6/15\n",
      "313/313 [==============================] - 465s 1s/step - loss: 0.4072 - accuracy: 0.8220 - val_loss: 0.4966 - val_accuracy: 0.8074\n",
      "Epoch 7/15\n",
      "313/313 [==============================] - 464s 1s/step - loss: 0.3947 - accuracy: 0.8223 - val_loss: 0.5478 - val_accuracy: 0.7953\n",
      "Epoch 8/15\n",
      "313/313 [==============================] - 467s 1s/step - loss: 0.3704 - accuracy: 0.8372 - val_loss: 0.4767 - val_accuracy: 0.8040\n",
      "Epoch 9/15\n",
      "313/313 [==============================] - 467s 1s/step - loss: 0.3367 - accuracy: 0.8579 - val_loss: 0.5230 - val_accuracy: 0.7988\n",
      "Epoch 10/15\n",
      "313/313 [==============================] - 468s 1s/step - loss: 0.3103 - accuracy: 0.8733 - val_loss: 0.4454 - val_accuracy: 0.8200\n",
      "Epoch 11/15\n",
      "313/313 [==============================] - 468s 1s/step - loss: 0.2782 - accuracy: 0.8824 - val_loss: 0.5876 - val_accuracy: 0.8022\n",
      "Epoch 12/15\n",
      "313/313 [==============================] - 468s 1s/step - loss: 0.2365 - accuracy: 0.9053 - val_loss: 0.5482 - val_accuracy: 0.7841\n",
      "Epoch 13/15\n",
      "313/313 [==============================] - 468s 1s/step - loss: 0.2076 - accuracy: 0.9191 - val_loss: 0.9850 - val_accuracy: 0.6939\n",
      "Epoch 14/15\n",
      "313/313 [==============================] - 466s 1s/step - loss: 0.1897 - accuracy: 0.9266 - val_loss: 0.7758 - val_accuracy: 0.7261\n",
      "Epoch 15/15\n",
      "313/313 [==============================] - 466s 1s/step - loss: 0.1607 - accuracy: 0.9390 - val_loss: 0.6106 - val_accuracy: 0.7913\n",
      "932/932 [==============================] - 205s 220ms/step\n",
      "[[9.6174198e-01 3.8257953e-02]\n",
      " [5.7357591e-01 4.2642409e-01]\n",
      " [9.9899858e-01 1.0014788e-03]\n",
      " ...\n",
      " [9.5643681e-01 4.3563213e-02]\n",
      " [9.9905604e-01 9.4393082e-04]\n",
      " [6.3675094e-01 3.6324906e-01]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "FITTING\n",
      "Epoch 1/15\n",
      "313/313 [==============================] - 476s 2s/step - loss: 0.7024 - accuracy: 0.5032 - val_loss: 0.6968 - val_accuracy: 0.2799\n",
      "Epoch 2/15\n",
      "313/313 [==============================] - 474s 2s/step - loss: 0.6932 - accuracy: 0.5052 - val_loss: 0.6896 - val_accuracy: 0.7201\n",
      "Epoch 3/15\n",
      "313/313 [==============================] - 471s 2s/step - loss: 0.6932 - accuracy: 0.5006 - val_loss: 0.6956 - val_accuracy: 0.2799\n",
      "Epoch 4/15\n",
      "313/313 [==============================] - 468s 1s/step - loss: 0.6932 - accuracy: 0.5020 - val_loss: 0.6913 - val_accuracy: 0.7201\n",
      "Epoch 5/15\n",
      "313/313 [==============================] - 469s 2s/step - loss: 0.6933 - accuracy: 0.4914 - val_loss: 0.6928 - val_accuracy: 0.7201\n",
      "Epoch 6/15\n",
      "313/313 [==============================] - 465s 1s/step - loss: 0.6932 - accuracy: 0.4962 - val_loss: 0.6896 - val_accuracy: 0.7201\n",
      "Epoch 7/15\n",
      "313/313 [==============================] - 469s 2s/step - loss: 0.6932 - accuracy: 0.4912 - val_loss: 0.6934 - val_accuracy: 0.2799\n",
      "Epoch 8/15\n",
      "313/313 [==============================] - 471s 2s/step - loss: 0.6932 - accuracy: 0.4872 - val_loss: 0.6935 - val_accuracy: 0.2799\n",
      "Epoch 9/15\n",
      "313/313 [==============================] - 471s 2s/step - loss: 0.6932 - accuracy: 0.4922 - val_loss: 0.6932 - val_accuracy: 0.2799\n",
      "Epoch 10/15\n",
      "313/313 [==============================] - 477s 2s/step - loss: 0.6932 - accuracy: 0.5010 - val_loss: 0.6883 - val_accuracy: 0.7201\n",
      "Epoch 11/15\n",
      "313/313 [==============================] - 475s 2s/step - loss: 0.6932 - accuracy: 0.5018 - val_loss: 0.6947 - val_accuracy: 0.2799\n",
      "Epoch 12/15\n",
      "313/313 [==============================] - 470s 2s/step - loss: 0.6932 - accuracy: 0.4994 - val_loss: 0.6958 - val_accuracy: 0.2799\n",
      "Epoch 13/15\n",
      "313/313 [==============================] - 470s 2s/step - loss: 0.6932 - accuracy: 0.4930 - val_loss: 0.6928 - val_accuracy: 0.7201\n",
      "Epoch 14/15\n",
      "313/313 [==============================] - 477s 2s/step - loss: 0.6932 - accuracy: 0.4980 - val_loss: 0.6926 - val_accuracy: 0.7201\n",
      "Epoch 15/15\n",
      "313/313 [==============================] - 471s 2s/step - loss: 0.6932 - accuracy: 0.4968 - val_loss: 0.6919 - val_accuracy: 0.7201\n",
      "932/932 [==============================] - 204s 219ms/step\n",
      "[[0.5014519  0.49854815]\n",
      " [0.5014519  0.49854815]\n",
      " [0.5014519  0.49854815]\n",
      " ...\n",
      " [0.5014519  0.49854815]\n",
      " [0.5014519  0.49854815]\n",
      " [0.5014519  0.49854815]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "FITTING\n",
      "Epoch 1/15\n",
      "313/313 [==============================] - 471s 2s/step - loss: 0.7043 - accuracy: 0.4912 - val_loss: 0.6909 - val_accuracy: 0.7201\n",
      "Epoch 2/15\n",
      "313/313 [==============================] - 466s 1s/step - loss: 0.6933 - accuracy: 0.5006 - val_loss: 0.6942 - val_accuracy: 0.2799\n",
      "Epoch 3/15\n",
      "313/313 [==============================] - 469s 2s/step - loss: 0.6933 - accuracy: 0.4940 - val_loss: 0.6938 - val_accuracy: 0.2799\n",
      "Epoch 4/15\n",
      "313/313 [==============================] - 470s 2s/step - loss: 0.6932 - accuracy: 0.4996 - val_loss: 0.6958 - val_accuracy: 0.2799\n",
      "Epoch 5/15\n",
      "313/313 [==============================] - 470s 2s/step - loss: 0.6932 - accuracy: 0.4974 - val_loss: 0.6949 - val_accuracy: 0.2799\n",
      "Epoch 6/15\n",
      "313/313 [==============================] - 470s 2s/step - loss: 0.6932 - accuracy: 0.4976 - val_loss: 0.6941 - val_accuracy: 0.2799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15\n",
      "313/313 [==============================] - 469s 1s/step - loss: 0.6933 - accuracy: 0.4904 - val_loss: 0.6951 - val_accuracy: 0.2799\n",
      "Epoch 8/15\n",
      "313/313 [==============================] - 466s 1s/step - loss: 0.6932 - accuracy: 0.5022 - val_loss: 0.6904 - val_accuracy: 0.7201\n",
      "Epoch 9/15\n",
      "313/313 [==============================] - 464s 1s/step - loss: 0.6932 - accuracy: 0.5042 - val_loss: 0.6951 - val_accuracy: 0.2799\n",
      "Epoch 10/15\n",
      "313/313 [==============================] - 471s 2s/step - loss: 0.6933 - accuracy: 0.4874 - val_loss: 0.6938 - val_accuracy: 0.2799\n",
      "Epoch 11/15\n",
      "313/313 [==============================] - 471s 2s/step - loss: 0.6932 - accuracy: 0.4944 - val_loss: 0.6934 - val_accuracy: 0.2799\n",
      "Epoch 12/15\n",
      "313/313 [==============================] - 469s 2s/step - loss: 0.6932 - accuracy: 0.4958 - val_loss: 0.6943 - val_accuracy: 0.2799\n",
      "Epoch 13/15\n",
      "313/313 [==============================] - 470s 2s/step - loss: 0.6932 - accuracy: 0.4970 - val_loss: 0.6960 - val_accuracy: 0.2799\n",
      "Epoch 14/15\n",
      "313/313 [==============================] - 470s 2s/step - loss: 0.6932 - accuracy: 0.4978 - val_loss: 0.6918 - val_accuracy: 0.7201\n",
      "Epoch 15/15\n",
      "313/313 [==============================] - 470s 2s/step - loss: 0.6932 - accuracy: 0.4916 - val_loss: 0.6935 - val_accuracy: 0.2799\n",
      "932/932 [==============================] - 203s 218ms/step\n",
      "[[0.49959877 0.5004012 ]\n",
      " [0.49959877 0.5004012 ]\n",
      " [0.49959877 0.5004012 ]\n",
      " ...\n",
      " [0.49959877 0.5004012 ]\n",
      " [0.49959877 0.5004012 ]\n",
      " [0.49959877 0.5004012 ]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "FITTING\n",
      "Epoch 1/15\n",
      "313/313 [==============================] - 469s 1s/step - loss: 92528168.0000 - accuracy: 0.5025 - val_loss: 0.6987 - val_accuracy: 0.2799\n",
      "Epoch 2/15\n",
      "313/313 [==============================] - 471s 2s/step - loss: 0.6977 - accuracy: 0.4972 - val_loss: 0.6738 - val_accuracy: 0.7201\n",
      "Epoch 3/15\n",
      "313/313 [==============================] - 468s 1s/step - loss: 0.6983 - accuracy: 0.5030 - val_loss: 0.6855 - val_accuracy: 0.7201\n",
      "Epoch 4/15\n",
      "313/313 [==============================] - 468s 1s/step - loss: 0.6976 - accuracy: 0.5018 - val_loss: 0.7278 - val_accuracy: 0.2799\n",
      "Epoch 5/15\n",
      "313/313 [==============================] - 468s 1s/step - loss: 0.6955 - accuracy: 0.5054 - val_loss: 0.6751 - val_accuracy: 0.7201\n",
      "Epoch 6/15\n",
      "313/313 [==============================] - 468s 1s/step - loss: 0.6961 - accuracy: 0.4926 - val_loss: 0.6639 - val_accuracy: 0.7201\n",
      "Epoch 7/15\n",
      "313/313 [==============================] - 468s 1s/step - loss: 0.6963 - accuracy: 0.4936 - val_loss: 0.6902 - val_accuracy: 0.7201\n",
      "Epoch 8/15\n",
      "313/313 [==============================] - 470s 2s/step - loss: 0.6947 - accuracy: 0.5040 - val_loss: 0.6996 - val_accuracy: 0.2799\n",
      "Epoch 9/15\n",
      "313/313 [==============================] - 472s 2s/step - loss: 0.6960 - accuracy: 0.4938 - val_loss: 0.7682 - val_accuracy: 0.2799\n",
      "Epoch 10/15\n",
      "313/313 [==============================] - 469s 1s/step - loss: 0.7333 - accuracy: 0.5050 - val_loss: 0.6429 - val_accuracy: 0.7201\n",
      "Epoch 11/15\n",
      "313/313 [==============================] - 468s 1s/step - loss: 0.7043 - accuracy: 0.4970 - val_loss: 0.7179 - val_accuracy: 0.2799\n",
      "Epoch 12/15\n",
      "313/313 [==============================] - 467s 1s/step - loss: 0.6966 - accuracy: 0.4948 - val_loss: 0.6916 - val_accuracy: 0.7201\n",
      "Epoch 13/15\n",
      "313/313 [==============================] - 464s 1s/step - loss: 0.6947 - accuracy: 0.4998 - val_loss: 0.7175 - val_accuracy: 0.2799\n",
      "Epoch 14/15\n",
      "313/313 [==============================] - 471s 2s/step - loss: 0.6946 - accuracy: 0.5048 - val_loss: 0.6723 - val_accuracy: 0.7201\n",
      "Epoch 15/15\n",
      "313/313 [==============================] - 476s 2s/step - loss: 0.6947 - accuracy: 0.4958 - val_loss: 0.6691 - val_accuracy: 0.7201\n",
      "932/932 [==============================] - 206s 221ms/step\n",
      "[[0.5292362  0.47076377]\n",
      " [0.5292362  0.47076377]\n",
      " [0.5292362  0.47076377]\n",
      " ...\n",
      " [0.5292362  0.47076374]\n",
      " [0.5292362  0.47076374]\n",
      " [0.5292362  0.47076374]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "FITTING\n",
      "Epoch 1/15\n",
      "313/313 [==============================] - 467s 1s/step - loss: 17490506.0000 - accuracy: 0.5039 - val_loss: 0.6803 - val_accuracy: 0.7201\n",
      "Epoch 2/15\n",
      "313/313 [==============================] - 465s 1s/step - loss: 0.6975 - accuracy: 0.4984 - val_loss: 0.7243 - val_accuracy: 0.2799\n",
      "Epoch 3/15\n",
      "313/313 [==============================] - 473s 2s/step - loss: 0.6957 - accuracy: 0.5048 - val_loss: 0.7254 - val_accuracy: 0.2799\n",
      "Epoch 4/15\n",
      "313/313 [==============================] - 468s 1s/step - loss: 0.6956 - accuracy: 0.5052 - val_loss: 0.6837 - val_accuracy: 0.7201\n",
      "Epoch 5/15\n",
      "313/313 [==============================] - 472s 2s/step - loss: 0.6953 - accuracy: 0.5012 - val_loss: 0.6881 - val_accuracy: 0.7201\n",
      "Epoch 6/15\n",
      "313/313 [==============================] - 475s 2s/step - loss: 0.6970 - accuracy: 0.4948 - val_loss: 0.6835 - val_accuracy: 0.7201\n",
      "Epoch 7/15\n",
      "313/313 [==============================] - 473s 2s/step - loss: 0.6954 - accuracy: 0.5004 - val_loss: 0.7008 - val_accuracy: 0.2799\n",
      "Epoch 8/15\n",
      "313/313 [==============================] - 470s 2s/step - loss: 0.6950 - accuracy: 0.4992 - val_loss: 0.6959 - val_accuracy: 0.2799\n",
      "Epoch 9/15\n",
      "313/313 [==============================] - 475s 2s/step - loss: 0.6946 - accuracy: 0.5008 - val_loss: 0.6838 - val_accuracy: 0.7201\n",
      "Epoch 10/15\n",
      "313/313 [==============================] - 470s 2s/step - loss: 0.6941 - accuracy: 0.5076 - val_loss: 0.6982 - val_accuracy: 0.2799\n",
      "Epoch 11/15\n",
      "313/313 [==============================] - 475s 2s/step - loss: 0.6949 - accuracy: 0.4940 - val_loss: 0.6961 - val_accuracy: 0.2799\n",
      "Epoch 12/15\n",
      "313/313 [==============================] - 477s 2s/step - loss: 0.6943 - accuracy: 0.4962 - val_loss: 0.6908 - val_accuracy: 0.7201\n",
      "Epoch 13/15\n",
      "313/313 [==============================] - 478s 2s/step - loss: 0.6947 - accuracy: 0.4984 - val_loss: 0.6624 - val_accuracy: 0.7201\n",
      "Epoch 14/15\n",
      "313/313 [==============================] - 478s 2s/step - loss: 0.6958 - accuracy: 0.4976 - val_loss: 0.7014 - val_accuracy: 0.2799\n",
      "Epoch 15/15\n",
      "313/313 [==============================] - 479s 2s/step - loss: 0.6947 - accuracy: 0.4954 - val_loss: 0.7123 - val_accuracy: 0.2799\n",
      "932/932 [==============================] - 207s 222ms/step\n",
      "[[0.47920644 0.5207936 ]\n",
      " [0.47920644 0.5207936 ]\n",
      " [0.47920644 0.5207936 ]\n",
      " ...\n",
      " [0.47920644 0.5207936 ]\n",
      " [0.47920644 0.5207936 ]\n",
      " [0.4792064  0.52079356]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "### #Learning rates above .01 are not useful\n",
    "import keras.backend as K\n",
    "import tensorflow_addons as tfa\n",
    "from keras.utils.np_utils import to_categorical  \n",
    "\n",
    "learning_rates = [.0001, .001, .01]\n",
    "batchsizes = [32, 256]\n",
    "mccs = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for bs in batchsizes: \n",
    "        model = VGG16(weights=None, include_top=True, input_shape= (50, 50,3), classes=2, classifier_activation='softmax')\n",
    "        adam = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer= adam, loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "        print(\"FITTING\")\n",
    "        history = model.fit(train_param_iterator, epochs=15, batch_size = bs, validation_data=val_iterator)\n",
    "    \n",
    "        y_pred = model.predict(testx_iterator)\n",
    "        print(y_pred)\n",
    "        y_true = to_categorical(np.array(y_val1), num_classes=2)\n",
    "        print(y_true)\n",
    "        metric = tfa.metrics.MatthewsCorrelationCoefficient(num_classes=2)\n",
    "        metric.update_state(y_true, y_pred)\n",
    "        result = metric.result()\n",
    "        result.numpy()\n",
    "        mccs.append((lr, bs, result.numpy()))\n",
    "\n",
    "\n",
    "# The model outputs a value between 0 and 1, which can be thought of as a probability of being a cancer or not\n",
    "# model.predict to find the pseudo probability which is a measure of confidence\n",
    "# we can output the score and the prediction\n",
    "# activation map to find the feature\n",
    "\n",
    "\n",
    "# LR = .0001 achieves the highest validation accuracy\n",
    "# Increase number of epochs to see if more epochs further increases val error\n",
    "# LR = .0001\n",
    "# model = VGG16(weights=None, include_top=True, input_shape= (50, 50,3), classes=2, classifier_activation='softmax')\n",
    "# adam = Adam(learning_rate=LR)\n",
    "# model.compile(optimizer= adam, loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=['accuracy', tfa.metrics.MatthewsCorrelationCoefficient(num_classes=2)])\n",
    "\n",
    "# print(\"FITTING\")j\n",
    "# history = model.fit(train_iterator, epochs=15, validation_data=val_iterator)\n",
    "# plt.plot(history.history['accuracy'], label='accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.ylim([0.5, 1])\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.title(\"Learning Rate: \" + str(LR))\n",
    "# plt.show()\n",
    "\n",
    "# model.save(\"FullTrain2023\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0001, 32, 0.49459004),\n",
       " (0.0001, 256, 0.5371189),\n",
       " (0.001, 32, 0.0),\n",
       " (0.001, 256, 0.0),\n",
       " (0.01, 32, 0.0),\n",
       " (0.01, 256, 0.0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29811, 2)\n"
     ]
    }
   ],
   "source": [
    "y_true = to_categorical(np.array(y_val1), num_classes=2)\n",
    "print(y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model('April1AB20k2023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iterator = datagen.flow(np.array(x_test), np.array(y_test))\n",
    "\n",
    "model.evaluate(test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testx_iterator = datagen.flow(np.array(x_test),batch_size=32, shuffle=False)\n",
    "y_prediction = model.predict(testx_iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.zeros(y_prediction.shape[0])\n",
    "mask = (y_prediction[:,0]<0.5)\n",
    "prediction[mask] = 1\n",
    "print(prediction)\n",
    "\n",
    "equal = np.zeros(y_prediction.shape[0])\n",
    "equal[prediction == np.array(y_test)] = 1\n",
    "\n",
    "accuracy = np.sum(equal)/y_prediction.shape[0]\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[np.array(y_test) == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "result = confusion_matrix(y_test, prediction , normalize='pred')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=result )\n",
    "\n",
    "disp.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cancer evaluation\n",
    "cancer_iterator = datagen.flow(np.array(np.array(x_test)[np.array(y_test) == 1]), np.array(y_test)[np.array(y_test) == 1])\n",
    "\n",
    "model.evaluate(cancer_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# benign evaluation\n",
    "\n",
    "benign_iterator = datagen.flow(np.array(np.array(x_test)[np.array(y_test) == 0]), np.array(y_test)[np.array(y_test) == 0])\n",
    "\n",
    "model.evaluate(benign_iterator)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## the penalty of missing a cancer is 2 million dollars * .05 100000\n",
    "## the penalty of diagnosing a cancer when it isn't there is lets say 20000\n",
    "import keras\n",
    "def get_img_array(img_path, size):\n",
    "    # `img` is a PIL image of size 299x299\n",
    "    img = load_img(img_path, target_size=size)\n",
    "    # `array` is a float32 Numpy array of shape (299, 299, 3)\n",
    "    array = tf.keras.utils.img_to_array(img)\n",
    "    # We add a dimension to transform our array into a \"batch\"\n",
    "    # of size (1, 299, 299, 3)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()\n",
    "plt.figure()\n",
    "img_size = (50, 50)\n",
    "path = \"imagedata/10253/0/10253_idx5_x1001_y1001_class0.png\"\n",
    "heatmap = make_gradcam_heatmap(datagen.flow(get_img_array(path, img_size))[0], model, \"block3_conv3\")\n",
    "plt.matshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n",
    "    # Load the original image\n",
    "    img = load_img(img_path)\n",
    "    img = tf.keras.utils.img_to_array(img)\n",
    "\n",
    "    # Rescale heatmap to a range 0-255\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    # Use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    # Create an image with RGB colorized heatmap\n",
    "    jet_heatmap = tf.keras.utils.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "\n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = jet_heatmap * alpha + img\n",
    "    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "\n",
    "    # Save the superimposed image\n",
    "    superimposed_img.save(cam_path)\n",
    "\n",
    "    # Display Grad CAM\n",
    "    display(Image(cam_path))\n",
    "\n",
    "\n",
    "save_and_display_gradcam(path, heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred_keras = model.predict(np.array(x_test)).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "y_pred0 = [y_prediction[i][1] for i in range(0, y_prediction.shape[0])]\n",
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(np.array(y_test), y_pred0)\n",
    "\n",
    "\n",
    "auc_keras = auc(fpr_keras, tpr_keras)\n",
    "plt.figure()\n",
    "\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(\"ROC.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(x_test).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add_metric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
